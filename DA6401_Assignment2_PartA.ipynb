{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937,
          "referenced_widgets": [
            "31ce035f74b047ef8c1a541987ba7e4b",
            "35564131b060430eb6aad44092d191da",
            "74e0ecca65ed4bd383224996de09a879",
            "e9878302fd1849ed9cea8449ed6a527b",
            "a1adbbb5e1044177b550ca2dc8898d7f",
            "d076cdaa2bb344a29355440618742131",
            "7d2f82c18b95475b915bb729422ea6dd",
            "c5f1a2c351794f67ae6b154174eec9f8",
            "ece166b4e2f64c7284ef3d61437d6192",
            "3d6e0d2d0287401b8bae64a63fa3423b",
            "22c33e4023d5452eb5d389b9bea9c683",
            "1a7e55d107d24af38e499ebc1f497054",
            "9f5ce548eca74a7f87d47866046c4661",
            "e25c7b23dff846799a46e907e38e7bd0",
            "7833f6426db74d4396830bd562093cbb",
            "c1236ac71525456f96bc6b7815758d48",
            "02ec5f93a9a54489a76ea0e001e5c7d0",
            "1a1ff487c78f4b739ba1b464d3f781b2",
            "6681d92abdba4a41b6280da1c8fd12a7",
            "c7f5d45ff82e4df28b2df5c1c159105c",
            "8b7337e5d01b4c7dabc020ec32f22c3a",
            "1aedaf7a3c3442c7bd16602d3177713b",
            "bd1b342808964a258d5a722841d02476",
            "8682bf18e133462d9914601342d9f235",
            "d31a44405bbe4c4fb41461fc61d662a3",
            "70888f1e1c2144818236fd5155da6011",
            "4f2801856a2346e897df48101925102e",
            "ec19bbd987984828b009a70830d2960d",
            "5c640dcc2997453faa22be7b2d2b2e9e",
            "10c085b6dcb14811802d7c44c71ed4ef",
            "de8e5534ac9a4a36abd7c1303bdf11ea",
            "ff80ea17d3bd4719923d2e92dc38b088",
            "cf2fd25f0f724baf8bd888ab6f40e712",
            "01f344583ce84aee8a982a6897d5fd0c",
            "c9ee823c82a44e7c9994c1e2f265fdf0",
            "7dc979dad6ae4ae1958f72f24fdecdff",
            "011a9af3250a4833b304a62e9358db13",
            "34b4e205a0d64b77bdd214828d96ebd4",
            "d9881904af6743baa27d956a466d0a62",
            "e7245cdb1626443087f7983e26c17122",
            "56bbf138ee414475ac9cda5f07a85fbe",
            "507ec2be35f04e42b342faf8026c6348",
            "22f24a8208f04ac3a9b1fa71352e476d",
            "2af7c3dd1b8e4e51a27149e47d18acc2",
            "ddffcf1a04c542449f600cf02ca620ef",
            "91337b1f189b418faf6fca661375608f",
            "dc147849aaf741eca837d186261447c5",
            "382be6ad26dc43cdb1b109233d5ddaf2",
            "77fe43ec2591405fb9fcd88c1fc274fb",
            "142138b2b1434882a5d8d388132b2de5",
            "8556f6b26a814759b727059f3c711c11",
            "456a8431785b499c92d5bb8946afc43a",
            "d0cfea954a734364a12bc3e9c260f253",
            "c5d98187c12d4d69870b190ef5a97687",
            "dc44603189b04dfaa9f109eb73b1ede9"
          ]
        },
        "id": "nGmOWTLcoyUu",
        "outputId": "ac0e34c1-3816-47c3-b722-3adc2e5f3655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250419_020109-h5x9yjst</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA/runs/h5x9yjst' target=\"_blank\">effortless-leaf-30</a></strong> to <a href='https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA' target=\"_blank\">https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA/runs/h5x9yjst' target=\"_blank\">https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA/runs/h5x9yjst</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Number of classes: 10\n",
            "Total parameters: 26,347,786\n",
            "Trainable parameters: 26,347,786\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1:   0%|          | 0/1250 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31ce035f74b047ef8c1a541987ba7e4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Train Loss: 2.7103, Train Acc: 11.62%, Val Loss: 2.2664, Val Acc: 13.50%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2:   0%|          | 0/1250 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a7e55d107d24af38e499ebc1f497054"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 - Train Loss: 2.2534, Train Acc: 14.18%, Val Loss: 2.2098, Val Acc: 16.75%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 3:   0%|          | 0/1250 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd1b342808964a258d5a722841d02476"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 - Train Loss: 2.2249, Train Acc: 16.09%, Val Loss: 2.1910, Val Acc: 16.35%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 4:   0%|          | 0/1250 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01f344583ce84aee8a982a6897d5fd0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 - Train Loss: 2.1833, Train Acc: 17.75%, Val Loss: 2.1043, Val Acc: 23.15%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 5:   0%|          | 0/1250 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddffcf1a04c542449f600cf02ca620ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 - Train Loss: 2.1021, Train Acc: 21.60%, Val Loss: 1.9998, Val Acc: 26.70%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅█</td></tr><tr><td>train_loss</td><td>█▃▂▂▁</td></tr><tr><td>val_accuracy</td><td>▁▃▃▆█</td></tr><tr><td>val_loss</td><td>█▇▆▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_accuracy</td><td>26.7</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>train_accuracy</td><td>21.59966</td></tr><tr><td>train_loss</td><td>2.10207</td></tr><tr><td>val_accuracy</td><td>26.7</td></tr><tr><td>val_loss</td><td>1.99981</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">effortless-leaf-30</strong> at: <a href='https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA/runs/h5x9yjst' target=\"_blank\">https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA/runs/h5x9yjst</a><br> View project at: <a href='https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA' target=\"_blank\">https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250419_020109-h5x9yjst/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torchvision import transforms, datasets\n",
        "from tqdm.notebook import tqdm\n",
        "import wandb\n",
        "\n",
        "# Mount Google Drive if in Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    DATA_ROOT = '/content/drive/MyDrive/inaturalist_12K'\n",
        "except ImportError:\n",
        "    DATA_ROOT = './data'\n",
        "\n",
        "# ---------------------------\n",
        "# Data Manager\n",
        "# ---------------------------\n",
        "class ImageDataManager:\n",
        "    \"\"\"Handles image dataset preparation: loading, normalization, augmentation\"\"\"\n",
        "    def __init__(self, img_size, data_root, device, standardize=True):\n",
        "        self.img_size = img_size\n",
        "        self.data_root = data_root\n",
        "        self.device = device\n",
        "        self.standardize = standardize\n",
        "        self.mean, self.std = None, None\n",
        "        # Store classes for later reference\n",
        "        self.classes = None\n",
        "\n",
        "    def _compute_stats(self, subset):\n",
        "        path = os.path.join(self.data_root, subset)\n",
        "        trans = transforms.Compose([transforms.Resize(self.img_size), transforms.ToTensor()])\n",
        "        if not os.path.isdir(path):\n",
        "            raise FileNotFoundError(f\"Dataset path not found: {path}\")\n",
        "        dataset = datasets.ImageFolder(path, trans)\n",
        "        loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "        mean = torch.zeros(3).to(self.device)\n",
        "        var = torch.zeros(3).to(self.device)\n",
        "        total = 0\n",
        "        for imgs, _ in loader:\n",
        "            imgs = imgs.to(self.device)\n",
        "            b = imgs.size(0)\n",
        "            imgs_flat = imgs.view(b, 3, -1)\n",
        "            mean += imgs_flat.mean(2).sum(0)\n",
        "            var += imgs_flat.var(2).sum(0)\n",
        "            total += b\n",
        "        self.mean = (mean/total).cpu()\n",
        "        self.std = (torch.sqrt(var/total)).cpu()\n",
        "        return self.mean, self.std\n",
        "\n",
        "    def create_loader(self, subset, batch_size=32, augmentations=None):\n",
        "        path = os.path.join(self.data_root, subset)\n",
        "        if not os.path.isdir(path):\n",
        "            raise FileNotFoundError(f\"Dataset path not found: {path}\")\n",
        "\n",
        "        transforms_list = [transforms.Resize(self.img_size), transforms.ToTensor()]\n",
        "        if self.standardize:\n",
        "            if self.mean is None:\n",
        "                self._compute_stats(subset)\n",
        "            transforms_list.append(transforms.Normalize(self.mean, self.std))\n",
        "\n",
        "        # Create a base dataset first to get classes\n",
        "        base_dataset = datasets.ImageFolder(path, transforms.Compose(transforms_list))\n",
        "\n",
        "        # Store classes for later reference\n",
        "        if self.classes is None:\n",
        "            self.classes = base_dataset.classes\n",
        "\n",
        "        # Handle augmentations correctly\n",
        "        if subset == 'train' and augmentations:\n",
        "            datasets_list = []\n",
        "            # Add dataset with no augmentation\n",
        "            datasets_list.append(base_dataset)\n",
        "\n",
        "            # Add datasets with each augmentation\n",
        "            for aug in augmentations:\n",
        "                # Create a new transforms list that includes the augmentation at the beginning\n",
        "                aug_transforms = [aug] + transforms_list\n",
        "                datasets_list.append(datasets.ImageFolder(path, transforms.Compose(aug_transforms)))\n",
        "\n",
        "            final_dataset = ConcatDataset(datasets_list)\n",
        "        else:\n",
        "            final_dataset = base_dataset\n",
        "\n",
        "        return DataLoader(\n",
        "            final_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=(subset == 'train'),\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "# ---------------------------\n",
        "# CNN Model\n",
        "# ---------------------------\n",
        "class CustomCNN(nn.Module):\n",
        "    \"\"\"Flexible CNN with conv, pool, batch-norm, dropout and dense layers\"\"\"\n",
        "    def __init__(self, input_size, in_channels, num_classes,\n",
        "                 conv_layers, dense_units,\n",
        "                 conv_activation=nn.ReLU, fc_activation=nn.ReLU,\n",
        "                 use_bn=True, dropout_rate=0.0):\n",
        "        super().__init__()\n",
        "        h, w = input_size\n",
        "        c = in_channels\n",
        "        self.features = nn.Sequential()\n",
        "        idx = 0\n",
        "        for cfg in conv_layers:\n",
        "            if cfg['type'] == 'conv':\n",
        "                self.features.add_module(\n",
        "                    f\"conv{idx}\",\n",
        "                    nn.Conv2d(c, cfg['filters'], cfg['kernel'], cfg['stride'], cfg['padding'])\n",
        "                )\n",
        "                if use_bn:\n",
        "                    self.features.add_module(f\"bn{idx}\", nn.BatchNorm2d(cfg['filters']))\n",
        "                self.features.add_module(f\"act{idx}\", conv_activation())\n",
        "                c = cfg['filters']\n",
        "                h = (h - cfg['kernel'] + 2*cfg['padding'])//cfg['stride'] + 1\n",
        "                w = (w - cfg['kernel'] + 2*cfg['padding'])//cfg['stride'] + 1\n",
        "            elif cfg['type'] == 'pool':\n",
        "                self.features.add_module(\n",
        "                    f\"pool{idx}\", nn.MaxPool2d(cfg['size'], cfg['stride'])\n",
        "                )\n",
        "                h = (h - cfg['size'])//cfg['stride'] + 1\n",
        "                w = (w - cfg['size'])//cfg['stride'] + 1\n",
        "            idx += 1\n",
        "        flat_dim = h * w * c\n",
        "        layers = []\n",
        "        in_feat = flat_dim\n",
        "        for u in dense_units:\n",
        "            layers.append(nn.Linear(in_feat, u))\n",
        "            layers.append(fc_activation())\n",
        "            if dropout_rate > 0:\n",
        "                layers.append(nn.Dropout(dropout_rate))\n",
        "            in_feat = u\n",
        "        layers.append(nn.Linear(in_feat, num_classes))\n",
        "        self.classifier = nn.Sequential(*layers)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# ---------------------------\n",
        "# Experiment Pipeline\n",
        "# ---------------------------\n",
        "class DLExperiment:\n",
        "    \"\"\"Handles training and validation only\"\"\"\n",
        "    def __init__(self, img_size, data_root, device=None, use_wandb=False):\n",
        "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.data_mgr = ImageDataManager(img_size, data_root, self.device)\n",
        "        self.use_wandb = use_wandb\n",
        "        self.train_loader = None\n",
        "        self.val_loader = None\n",
        "        self.model = None\n",
        "\n",
        "    def setup_data(self, batch_size, augmentations=None):\n",
        "        self.train_loader = self.data_mgr.create_loader('train', batch_size, augmentations)\n",
        "        self.val_loader = self.data_mgr.create_loader('val', batch_size)\n",
        "        if not self.train_loader or not self.val_loader:\n",
        "            raise FileNotFoundError(\"Both 'train' and 'val' directories must exist under data_root.\")\n",
        "\n",
        "    def setup_model(self, conv_config, dense_units, num_classes,\n",
        "                    use_bn=True, dropout_rate=0.0):\n",
        "        self.model = CustomCNN(\n",
        "            input_size=self.data_mgr.img_size,\n",
        "            in_channels=3,\n",
        "            num_classes=num_classes,\n",
        "            conv_layers=conv_config,\n",
        "            dense_units=dense_units,\n",
        "            use_bn=use_bn,\n",
        "            dropout_rate=dropout_rate\n",
        "        ).to(self.device)\n",
        "\n",
        "    def train(self, epochs, lr, weight_decay):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not initialized. Call setup_model() first.\")\n",
        "\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        best_acc = 0.0\n",
        "\n",
        "        for epoch in range(1, epochs+1):\n",
        "            # Training\n",
        "            self.model.train()\n",
        "            total, correct = 0, 0\n",
        "            train_loss = 0.0\n",
        "\n",
        "            for imgs, labels in tqdm(self.train_loader, desc=f\"Epoch {epoch}\"):\n",
        "                imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item() * labels.size(0)\n",
        "                _, preds = outputs.max(1)\n",
        "                correct += preds.eq(labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "            # Calculate metrics\n",
        "            train_loss = train_loss / total\n",
        "            train_acc = 100 * correct / total\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_acc = self.evaluate()\n",
        "\n",
        "            # Log metrics to wandb\n",
        "            if self.use_wandb:\n",
        "                wandb.log({\n",
        "                    \"epoch\": epoch,\n",
        "                    \"train_loss\": train_loss,\n",
        "                    \"train_accuracy\": train_acc,\n",
        "                    \"val_loss\": val_loss,\n",
        "                    \"val_accuracy\": val_acc\n",
        "                })\n",
        "\n",
        "            print(f\"Epoch {epoch}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "            # Save best model\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
        "                if self.use_wandb:\n",
        "                    wandb.run.summary[\"best_val_accuracy\"] = best_acc\n",
        "\n",
        "        return best_acc\n",
        "\n",
        "    def evaluate(self):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not initialized. Call setup_model() first.\")\n",
        "\n",
        "        self.model.eval()\n",
        "        total, correct = 0, 0\n",
        "        loss_sum = 0.0\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in self.val_loader:\n",
        "                imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
        "                outputs = self.model(imgs)\n",
        "                loss_sum += criterion(outputs, labels).item() * labels.size(0)\n",
        "                _, preds = outputs.max(1)\n",
        "                correct += preds.eq(labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        return loss_sum/total, 100*correct/total\n",
        "\n",
        "# ---------------------------\n",
        "# Main Execution\n",
        "# ---------------------------\n",
        "def run_experiment(config):\n",
        "    wandb.init(project=\"DA6401_Assignment2_PartA\", config=config)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    exp = DLExperiment(img_size=(config['crop_size'], config['crop_size']),\n",
        "                       data_root=DATA_ROOT,\n",
        "                       device=device,\n",
        "                       use_wandb=True)\n",
        "\n",
        "    # Setup data with proper augmentations\n",
        "    augmentations = []\n",
        "    if config.get('augmentations'):\n",
        "        for aug in config['augmentations']:\n",
        "            augmentations.append(aug)\n",
        "\n",
        "    exp.setup_data(batch_size=config['batch_size'], augmentations=augmentations)\n",
        "\n",
        "    # Build conv config\n",
        "    conv_cfg = []\n",
        "    filters = config['num_filters']\n",
        "    for i in range(config['conv_layers']):\n",
        "        conv_cfg.append({'type': 'conv', 'filters': filters,\n",
        "                         'kernel': config['filter_size'], 'stride': 1,\n",
        "                         'padding': config['filter_size']//2})\n",
        "        conv_cfg.append({'type': 'pool', 'size': 2, 'stride': 2})\n",
        "\n",
        "        # Apply filter growth strategy\n",
        "        if config.get('filter_growth_strategy') == 'double':\n",
        "            filters *= 2\n",
        "        elif config.get('filter_growth_strategy') == 'half':\n",
        "            filters = max(16, filters // 2)  # Prevent filters from becoming too small\n",
        "        # Default is to use filter_growth_factor\n",
        "        else:\n",
        "            filters = int(filters * config['filter_growth_factor'])\n",
        "\n",
        "    # Get number of classes from the data manager\n",
        "    num_classes = len(exp.data_mgr.classes)\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    # Setup model\n",
        "    exp.setup_model(\n",
        "        conv_cfg,\n",
        "        dense_units=[config['hidden_units']] * config['dense_layers'],\n",
        "        num_classes=num_classes,\n",
        "        use_bn=config['batch_norm'],\n",
        "        dropout_rate=config['dropout_rate']\n",
        "    )\n",
        "\n",
        "    # Report model parameters\n",
        "    total_params = sum(p.numel() for p in exp.model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in exp.model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Train the model\n",
        "    best_accuracy = exp.train(\n",
        "        epochs=config['training_epochs'],\n",
        "        lr=config['learning_rate'],\n",
        "        weight_decay=config['l2_regularization']\n",
        "    )\n",
        "\n",
        "    # Close wandb run\n",
        "    wandb.finish()\n",
        "\n",
        "    return best_accuracy\n",
        "\n",
        "# Example config with corrected augmentations\n",
        "default_config = {\n",
        "    'conv_layers': 4,\n",
        "    'num_filters': 32,\n",
        "    'filter_size': 3,\n",
        "    'filter_growth_factor': 2,\n",
        "    'filter_growth_strategy': 'double',  # Options: 'double', 'half', None (use factor)\n",
        "    'dense_layers': 2,\n",
        "    'hidden_units': 512,\n",
        "    'batch_norm': True,\n",
        "    'dropout_rate': 0.2,\n",
        "    # Each augmentation is separate now\n",
        "    'augmentations': [\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1)\n",
        "    ],\n",
        "    'crop_size': 224,  # Reduced from 600 to save memory\n",
        "    'batch_size': 32,\n",
        "    'learning_rate': 1e-3,\n",
        "    'l2_regularization': 1e-4,\n",
        "    'training_epochs': 10\n",
        "}\n",
        "\n",
        "# For setting up hyperparameter sweeps\n",
        "def sweep_configuration():\n",
        "    sweep_config = {\n",
        "        'method': 'bayes',  # Can be 'grid', 'random' or 'bayes'\n",
        "        'metric': {\n",
        "            'name': 'val_accuracy',\n",
        "            'goal': 'maximize'\n",
        "        },\n",
        "        'parameters': {\n",
        "            'conv_layers': {\n",
        "                'values': [3, 4, 5]\n",
        "            },\n",
        "            'num_filters': {\n",
        "                'values': [16, 32, 64]\n",
        "            },\n",
        "            'filter_size': {\n",
        "                'values': [3, 5]\n",
        "            },\n",
        "            'filter_growth_strategy': {\n",
        "                'values': ['double', 'half', None]\n",
        "            },\n",
        "            'dense_layers': {\n",
        "                'values': [1, 2]\n",
        "            },\n",
        "            'hidden_units': {\n",
        "                'values': [256, 512, 1024]\n",
        "            },\n",
        "            'batch_norm': {\n",
        "                'values': [True, False]\n",
        "            },\n",
        "            'dropout_rate': {\n",
        "                'values': [0.0, 0.2, 0.3, 0.5]\n",
        "            },\n",
        "            'learning_rate': {\n",
        "                'values': [1e-4, 3e-4, 1e-3]\n",
        "            },\n",
        "            'batch_size': {\n",
        "                'values': [16, 32, 64]\n",
        "            },\n",
        "            'crop_size': {\n",
        "                'values': [224]  # Fixed for memory efficiency\n",
        "            },\n",
        "            'training_epochs': {\n",
        "                'value': 30  # Fixed for all runs\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return sweep_config\n",
        "\n",
        "# For running a sweep\n",
        "def run_sweep():\n",
        "    sweep_id = wandb.sweep(sweep_configuration(), project=\"DA6401_Assignment2_PartA\")\n",
        "    wandb.agent(sweep_id, function=run_experiment, count=20)  # Run 20 experiments\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Choose one of these:\n",
        "    run_experiment(default_config)  # Run a single experiment with default config\n",
        "    # run_sweep()  # Run hyperparameter sweep"
      ]
    }
  ]
}
