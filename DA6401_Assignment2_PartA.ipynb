{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNor5GblgYhfYoPzlhJeyQo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiRajesh228/DA6401_Assignment2/blob/main/DA6401_Assignment2_PartA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "nGmOWTLcoyUu",
        "outputId": "089dd52d-6467-43fe-95e3-accdb68002b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">tough-fog-1</strong> at: <a href='https://wandb.ai/cs24m040-iit-madras/inaturalist_experiment/runs/5o4lgdaf' target=\"_blank\">https://wandb.ai/cs24m040-iit-madras/inaturalist_experiment/runs/5o4lgdaf</a><br> View project at: <a href='https://wandb.ai/cs24m040-iit-madras/inaturalist_experiment' target=\"_blank\">https://wandb.ai/cs24m040-iit-madras/inaturalist_experiment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250418_064438-5o4lgdaf/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250418_064947-v2l35w4n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA/runs/v2l35w4n' target=\"_blank\">fluent-capybara-1</a></strong> to <a href='https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA' target=\"_blank\">https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA/runs/v2l35w4n' target=\"_blank\">https://wandb.ai/cs24m040-iit-madras/DA6401_Assignment2_PartA/runs/v2l35w4n</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/inaturalist_12K/test'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1bfaf9e2f0ee>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-1bfaf9e2f0ee>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(config, sweep)\u001b[0m\n\u001b[1;32m    231\u001b[0m     exp = DLExperiment(img_size=(config['crop_size'],config['crop_size']),\n\u001b[1;32m    232\u001b[0m                        data_root=DATA_ROOT, device=dev, use_wandb=True)\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_aug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_aug'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mconv_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-1bfaf9e2f0ee>\u001b[0m in \u001b[0;36msetup_data\u001b[0;34m(self, batch_size, data_aug)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_presets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loader\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loader\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         ds = (self.train_loader.dataset if not isinstance(self.train_loader.dataset, ConcatDataset)\n\u001b[1;32m    159\u001b[0m               else self.train_loader.dataset.datasets[0])\n",
            "\u001b[0;32m<ipython-input-2-1bfaf9e2f0ee>\u001b[0m in \u001b[0;36mcreate_loader\u001b[0;34m(self, subset, batch_size, augmentations)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mfinal_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             final_ds = datasets.ImageFolder(\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/inaturalist_12K/test'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, ConcatDataset, RandomSampler\n",
        "from torchvision import transforms, datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import wandb\n",
        "\n",
        "# Mount Google Drive if in Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DATA_ROOT = '/content/drive/MyDrive/inaturalist_12K'\n",
        "except ImportError:\n",
        "    DATA_ROOT = './data'\n",
        "\n",
        "# ---------------------------\n",
        "# Data Manager\n",
        "# ---------------------------\n",
        "class ImageDataManager:\n",
        "    \"\"\"Handles image dataset preparation: loading, normalization, augmentation\"\"\"\n",
        "    def __init__(self, img_size, data_root, device, standardize=True):\n",
        "        self.img_size = img_size\n",
        "        self.data_root = data_root\n",
        "        self.device = device\n",
        "        self.standardize = standardize\n",
        "        self.mean, self.std = None, None\n",
        "\n",
        "    def _compute_stats(self, subset):\n",
        "        trans = transforms.Compose([transforms.Resize(self.img_size), transforms.ToTensor()])\n",
        "        dataset = datasets.ImageFolder(os.path.join(self.data_root, subset), trans)\n",
        "        loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "        mean = torch.zeros(3).to(self.device)\n",
        "        var = torch.zeros(3).to(self.device)\n",
        "        total = 0\n",
        "        for imgs, _ in loader:\n",
        "            imgs = imgs.to(self.device)\n",
        "            b = imgs.size(0)\n",
        "            imgs_flat = imgs.view(b, 3, -1)\n",
        "            mean += imgs_flat.mean(2).sum(0)\n",
        "            var  += imgs_flat.var(2).sum(0)\n",
        "            total += b\n",
        "        self.mean = (mean/total).cpu()\n",
        "        self.std  = (torch.sqrt(var/total)).cpu()\n",
        "        return self.mean, self.std\n",
        "\n",
        "    def create_loader(self, subset, batch_size=32, augmentations=None):\n",
        "        base = [transforms.Resize(self.img_size), transforms.ToTensor()]\n",
        "        if self.standardize:\n",
        "            if self.mean is None:\n",
        "                self._compute_stats(subset)\n",
        "            base.append(transforms.Normalize(self.mean, self.std))\n",
        "\n",
        "        trans_list = list(base)\n",
        "        if subset.startswith('train') and augmentations:\n",
        "            datasets_list = []\n",
        "            datasets_list.append(datasets.ImageFolder(\n",
        "                os.path.join(self.data_root, subset), transforms.Compose(trans_list)\n",
        "            ))\n",
        "            for aug in augmentations:\n",
        "                datasets_list.append(datasets.ImageFolder(\n",
        "                    os.path.join(self.data_root, subset), transforms.Compose(aug + trans_list)\n",
        "                ))\n",
        "            final_ds = ConcatDataset(datasets_list)\n",
        "        else:\n",
        "            final_ds = datasets.ImageFolder(\n",
        "                os.path.join(self.data_root, subset), transforms.Compose(trans_list)\n",
        "            )\n",
        "        return DataLoader(final_ds, batch_size=batch_size,\n",
        "                          shuffle=subset.startswith('train'),\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "# ---------------------------\n",
        "# CNN Model\n",
        "# ---------------------------\n",
        "class CustomCNN(nn.Module):\n",
        "    \"\"\"Flexible CNN with conv, pool, batch-norm, dropout and dense layers\"\"\"\n",
        "    def __init__(self, input_size, in_channels, num_classes,\n",
        "                 conv_layers, dense_units,\n",
        "                 conv_activation=nn.ReLU, fc_activation=nn.ReLU,\n",
        "                 use_bn=True, dropout_rate=0.0):\n",
        "        super().__init__()\n",
        "        h, w = input_size\n",
        "        current_c = in_channels\n",
        "        self.features = nn.Sequential()\n",
        "        layer_idx = 0\n",
        "        for cfg in conv_layers:\n",
        "            if cfg['type']=='conv':\n",
        "                self.features.add_module(f\"conv{layer_idx}\",\n",
        "                                         nn.Conv2d(current_c, cfg['filters'],\n",
        "                                                   cfg['kernel'], cfg['stride'], cfg['padding']))\n",
        "                if use_bn:\n",
        "                    self.features.add_module(f\"bn{layer_idx}\", nn.BatchNorm2d(cfg['filters']))\n",
        "                self.features.add_module(f\"act{layer_idx}\", conv_activation())\n",
        "                current_c = cfg['filters']\n",
        "                h = (h - cfg['kernel'] + 2*cfg['padding'])//cfg['stride'] + 1\n",
        "                w = (w - cfg['kernel'] + 2*cfg['padding'])//cfg['stride'] + 1\n",
        "            elif cfg['type']=='pool':\n",
        "                self.features.add_module(f\"pool{layer_idx}\",\n",
        "                                         nn.MaxPool2d(cfg['size'], cfg['stride']))\n",
        "                h = (h - cfg['size'])//cfg['stride'] + 1\n",
        "                w = (w - cfg['size'])//cfg['stride'] + 1\n",
        "            layer_idx +=1\n",
        "\n",
        "        flatten_dim = h * w * current_c\n",
        "        seq = []\n",
        "        in_feat = flatten_dim\n",
        "        for i, u in enumerate(dense_units):\n",
        "            seq.append(nn.Linear(in_feat, u))\n",
        "            seq.append(fc_activation())\n",
        "            if dropout_rate >0:\n",
        "                seq.append(nn.Dropout(dropout_rate))\n",
        "            in_feat = u\n",
        "        seq.append(nn.Linear(in_feat, num_classes))\n",
        "        self.classifier = nn.Sequential(*seq)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# ---------------------------\n",
        "# Experiment Pipeline\n",
        "# ---------------------------\n",
        "class DLExperiment:\n",
        "    \"\"\"Manages end-to-end training, evaluation, and visualization\"\"\"\n",
        "    def __init__(self, img_size, data_root, device=None, use_wandb=False):\n",
        "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.data_mgr = ImageDataManager(img_size, data_root, self.device)\n",
        "        self.use_wandb = use_wandb\n",
        "        self.class_names = None\n",
        "\n",
        "    def setup_data(self, batch_size, data_aug=None):\n",
        "        aug_presets = []\n",
        "        if data_aug:\n",
        "            aug_list = [ [transforms.RandomHorizontalFlip()],\n",
        "                         [transforms.ColorJitter(brightness=0.3)] ]\n",
        "            aug_presets = aug_list[:data_aug]\n",
        "        self.train_loader = self.data_mgr.create_loader('train', batch_size, augmentations=aug_presets)\n",
        "        self.val_loader   = self.data_mgr.create_loader('val',   batch_size)\n",
        "        self.test_loader  = self.data_mgr.create_loader('test',  batch_size)\n",
        "        ds = (self.train_loader.dataset if not isinstance(self.train_loader.dataset, ConcatDataset)\n",
        "              else self.train_loader.dataset.datasets[0])\n",
        "        self.class_names = ds.classes\n",
        "\n",
        "    def setup_model(self, conv_config, dense_units, num_classes,\n",
        "                    use_bn=True, dropout_rate=0.0):\n",
        "        self.model = CustomCNN(\n",
        "            input_size=self.data_mgr.img_size,\n",
        "            in_channels=3, num_classes=num_classes,\n",
        "            conv_layers=conv_config, dense_units=dense_units,\n",
        "            use_bn=use_bn, dropout_rate=dropout_rate\n",
        "        ).to(self.device)\n",
        "\n",
        "    def train(self, epochs, lr, weight_decay):\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        best_acc = 0.0\n",
        "        for epoch in range(1, epochs+1):\n",
        "            self.model.train()\n",
        "            total_loss=0; total=0; correct=0\n",
        "            for imgs, labels in self.train_loader:\n",
        "                imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                out = self.model(imgs)\n",
        "                loss = criterion(out, labels)\n",
        "                loss.backward(); optimizer.step()\n",
        "                total_loss += loss.item()*imgs.size(0)\n",
        "                _, pred = out.max(1); total+=labels.size(0)\n",
        "                correct += pred.eq(labels).sum().item()\n",
        "            train_acc = 100*correct/total\n",
        "            val_loss, val_acc = self.evaluate(self.val_loader)\n",
        "            print(f\"Epoch {epoch}/{epochs} - Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
        "            if val_acc>best_acc:\n",
        "                best_acc=val_acc\n",
        "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    def evaluate(self, loader):\n",
        "        self.model.eval(); total=0; correct=0; loss=0.0; crit=nn.CrossEntropyLoss()\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
        "                out = self.model(imgs)\n",
        "                loss += crit(out, labels).item()*imgs.size(0)\n",
        "                _, pred = out.max(1); total+=labels.size(0)\n",
        "                correct+=pred.eq(labels).sum().item()\n",
        "        return loss/total, 100*correct/total\n",
        "\n",
        "    def visualize(self, num_images=9):\n",
        "        self.model.eval()\n",
        "        sampler = RandomSampler(self.test_loader.dataset)\n",
        "        imgs, labels = next(iter(DataLoader(self.test_loader.dataset, batch_size=num_images, sampler=sampler)))\n",
        "        with torch.no_grad(): preds = self.model(imgs.to(self.device)).argmax(1)\n",
        "        plt.figure(figsize=(8,8))\n",
        "        for i in range(num_images):\n",
        "            plt.subplot(3,3,i+1)\n",
        "            im = imgs[i].permute(1,2,0).cpu().numpy()\n",
        "            plt.imshow(np.clip(im,0,1))\n",
        "            color = 'green' if preds[i]==labels[i] else 'red'\n",
        "            plt.title(f\"True:{self.class_names[labels[i]]}\\nPred:{self.class_names[preds[i]]}\", color=color)\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration and Execution\n",
        "# ---------------------------\n",
        "def run_experiment(config, sweep=False):\n",
        "    # Initialize wandb to the specific project\n",
        "    if sweep and wandb.run is None:\n",
        "        wandb.init(project=\"DA6401_Assignment2_PartA\", config=config)\n",
        "    elif not sweep:\n",
        "        wandb.init(project=\"DA6401_Assignment2_PartA\", config=config)\n",
        "\n",
        "    dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    exp = DLExperiment(img_size=(config['crop_size'],config['crop_size']),\n",
        "                       data_root=DATA_ROOT, device=dev, use_wandb=True)\n",
        "    exp.setup_data(batch_size=config['batch_size'], data_aug=config.get('data_aug',0))\n",
        "\n",
        "    conv_cfg = []\n",
        "    filters = config['num_filters']\n",
        "    for i in range(config['conv_layers']):\n",
        "        conv_cfg.append({'type':'conv','filters':filters,'kernel':config['filter_size'],\n",
        "                         'stride':1,'padding':config['filter_size']//2})\n",
        "        conv_cfg.append({'type':'pool','size':2,'stride':2})\n",
        "        filters *= config['filter_growth_factor']\n",
        "\n",
        "    exp.setup_model(conv_cfg,\n",
        "                    dense_units=[config['hidden_units']]*config['dense_layers'],\n",
        "                    num_classes=len(exp.class_names),\n",
        "                    use_bn=config['batch_norm'],\n",
        "                    dropout_rate=config.get('dropout_rate',0.0))\n",
        "\n",
        "    exp.train(epochs=config['training_epochs'],\n",
        "              lr=config['learning_rate'],\n",
        "              weight_decay=config['l2_regularization'])\n",
        "    exp.visualize()\n",
        "\n",
        "# Example config dict\n",
        "sample_config = {\n",
        "    'conv_layers': 4,\n",
        "    'num_filters': 32,\n",
        "    'filter_size': 3,\n",
        "    'filter_growth_factor': 2,\n",
        "    'dense_layers': 2,\n",
        "    'hidden_units': 512,\n",
        "    'batch_norm': True,\n",
        "    'dropout_rate': 0.2,\n",
        "    'data_aug': 2,\n",
        "    'crop_size': 600,\n",
        "    'batch_size': 32,\n",
        "    'learning_rate': 1e-3,\n",
        "    'l2_regularization': 1e-4,\n",
        "    'training_epochs': 10\n",
        "}\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_experiment(sample_config)\n"
      ]
    }
  ]
}