{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObgL/rcUtL7vhB+qquje29",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiRajesh228/DA6401_Assignment2/blob/main/DA6401_Assignment2_PartB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnWfZrs8t-hU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "import wandb\n",
        "\n",
        "import torch.optim as optims\n",
        "from torch.utils.data import Dataset, DataLoader,ChainDataset, ConcatDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class DatasetManager:\n",
        "\n",
        "    def __init__(self, root_path, computation_device, base_transforms=None):\n",
        "        self.root_path = root_path\n",
        "        self.device = computation_device\n",
        "        self.base_transforms = base_transforms or []\n",
        "\n",
        "    def build_loader(self, data_subset, batch_size=16, shuffle=True,\n",
        "                    workers=2, augmentation_pipelines=None, pin_memory=False):\n",
        "        \"\"\"\n",
        "        Constructs and returns a DataLoader for the specified dataset subset.\n",
        "\n",
        "        Parameters:\n",
        "            data_subset (str): Subdirectory name (e.g., 'train/', 'val/', 'test/')\n",
        "            batch_size (int): Number of samples per batch\n",
        "            shuffle (bool): Whether to shuffle the data\n",
        "            workers (int): Number of parallel data loading processes\n",
        "            augmentation_pipelines (list): Optional list of augmentation sequences\n",
        "            pin_memory (bool): Enable fast data transfer to GPU\n",
        "\n",
        "        Returns:\n",
        "            DataLoader configured for the specified dataset\n",
        "        \"\"\"\n",
        "        print(f\"Initializing {data_subset} dataset processing...\")\n",
        "\n",
        "        # Base transformations applied to all datasets\n",
        "        core_transforms = transforms.Compose(self.base_transforms)\n",
        "\n",
        "        # Handle dataset construction\n",
        "        if data_subset.startswith('train') and augmentation_pipelines:\n",
        "            main_set = self._create_dataset(data_subset, core_transforms)\n",
        "            augmented_sets = [self._create_dataset(data_subset, transforms.Compose(\n",
        "                self.base_transforms + pipeline)) for pipeline in augmentation_pipelines]\n",
        "            combined_data = ConcatDataset([main_set] + augmented_sets)\n",
        "        else:\n",
        "            combined_data = self._create_dataset(data_subset, core_transforms)\n",
        "\n",
        "        # Configure loader parameters\n",
        "        loader_config = {\n",
        "            'batch_size': batch_size,\n",
        "            'num_workers': workers,\n",
        "            'pin_memory': pin_memory,\n",
        "            'persistent_workers': workers > 0\n",
        "        }\n",
        "\n",
        "        # Shuffle configuration\n",
        "        if data_subset.startswith('train'):\n",
        "            loader_config['shuffle'] = shuffle\n",
        "            # For distributed training, replace with DistributedSampler\n",
        "            sampler = None\n",
        "        else:\n",
        "            loader_config['shuffle'] = False\n",
        "            sampler = None\n",
        "\n",
        "        return DataLoader(combined_data, sampler=sampler, **loader_config)\n",
        "\n",
        "    def _create_dataset(self, subset_path, transform_pipeline):\n",
        "        \"\"\"Helper to create ImageFolder dataset with specified transforms\"\"\"\n",
        "        full_path = f\"{self.root_path}/{subset_path}\"\n",
        "        return torchvision.datasets.ImageFolder(\n",
        "            root=full_path,\n",
        "            transform=transform_pipeline\n",
        "        )"
      ],
      "metadata": {
        "id": "dWRMhSSduMud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TrainingPipeline:\n",
        "    \"\"\"Handles complete model training workflow including data preparation, model setup, and training\"\"\"\n",
        "\n",
        "    def __init__(self, computation_device, data_root, use_wandb=False, kaggle_env=False):\n",
        "        self.device = computation_device\n",
        "        self.data_root = data_root\n",
        "        self.wandb_integration = use_wandb\n",
        "        self.kaggle_environment = kaggle_env\n",
        "        self.model = None\n",
        "        self.train_loader = None\n",
        "        self.val_loader = None\n",
        "        self.test_loader = None\n",
        "\n",
        "    def prepare_data_loaders(self, batch_size, shuffle_data, augmentation_transforms=None,\n",
        "                            worker_count=0, memory_pinning=False):\n",
        "        \"\"\"Initializes data loaders for all dataset splits\"\"\"\n",
        "\n",
        "        data_manager = DatasetManager(root_path=self.data_root,\n",
        "                                    computation_device=self.device,\n",
        "                                    base_transforms=self.preprocessing_pipeline)\n",
        "\n",
        "        memory_pinning = memory_pinning if self.device == \"cpu\" else True\n",
        "\n",
        "        self.train_loader = data_manager.build_loader(\n",
        "            \"train/\", batch_size, shuffle_data, worker_count,\n",
        "            augmentation_transforms, memory_pinning\n",
        "        )\n",
        "\n",
        "        self.val_loader = data_manager.build_loader(\n",
        "            \"validation/\", batch_size, False, worker_count,\n",
        "            pin_memory=False\n",
        "        )\n",
        "\n",
        "        self.test_loader = data_manager.build_loader(\n",
        "            \"test/\", batch_size, False, worker_count,\n",
        "            pin_memory=False\n",
        "        )\n",
        "\n",
        "        return self.train_loader, self.val_loader, self.test_loader\n",
        "\n",
        "    def initialize_resnet(self, output_classes):\n",
        "        \"\"\"Configures a pretrained ResNet50 model for transfer learning\"\"\"\n",
        "\n",
        "        self.model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "        self.preprocessing_pipeline = ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
        "\n",
        "        # Freeze base layers\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Modify final layer\n",
        "        final_in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Sequential(\n",
        "            nn.Linear(final_in_features, output_classes),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "\n",
        "        # Initialize final layer\n",
        "        nn.init.xavier_uniform_(self.model.fc[0].weight)\n",
        "        self.model.fc[0].bias.data.fill_(0.01)\n",
        "        self.model.fc[0].requires_grad_(True)\n",
        "\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def _calculate_metrics(self, model, data_loader):\n",
        "        \"\"\"Evaluates model performance on given dataset\"\"\"\n",
        "        model.eval()\n",
        "        correct_count = 0\n",
        "        total_samples = 0\n",
        "        running_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in data_loader:\n",
        "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                running_loss += self.loss_fn(outputs, targets).item() * inputs.size(0)\n",
        "                predictions = outputs.argmax(dim=1)\n",
        "\n",
        "                correct_count += (predictions == targets).sum().item()\n",
        "                total_samples += inputs.size(0)\n",
        "\n",
        "        accuracy = 100 * correct_count / total_samples\n",
        "        avg_loss = running_loss / total_samples\n",
        "        return round(avg_loss, 2), round(accuracy, 2)\n",
        "\n",
        "    def execute_training(self, learning_rate, l2_penalty, loss_type, optimizer_choice, num_epochs):\n",
        "        \"\"\"Executes the complete training process\"\"\"\n",
        "\n",
        "        self._configure_optimizer(learning_rate, l2_penalty, optimizer_choice)\n",
        "        self.loss_fn = nn.CrossEntropyLoss().to(self.device)\n",
        "\n",
        "        start_timestamp = time.time()\n",
        "\n",
        "        for epoch in tqdm(range(num_epochs)):\n",
        "            self.model.train()\n",
        "            epoch_correct = 0\n",
        "            epoch_total = 0\n",
        "            epoch_loss = 0.0\n",
        "\n",
        "            for inputs, labels in self.train_loader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.loss_fn(outputs, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item() * inputs.size(0)\n",
        "                predictions = outputs.argmax(dim=1)\n",
        "                epoch_correct += (predictions == labels).sum().item()\n",
        "                epoch_total += inputs.size(0)\n",
        "\n",
        "            train_acc = 100 * epoch_correct / epoch_total\n",
        "            train_loss = epoch_loss / epoch_total\n",
        "            val_loss, val_acc = self._calculate_metrics(self.model, self.val_loader)\n",
        "\n",
        "            if epoch % 5 == 0:\n",
        "                self._save_checkpoint()\n",
        "\n",
        "            if self.wandb_integration:\n",
        "                self._log_training_metrics(epoch, train_loss, train_acc, val_loss, val_acc)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}: \"\n",
        "                f\"Train Loss {train_loss:.2f} | Acc {train_acc:.2f}% | \"\n",
        "                f\"Val Loss {val_loss:.2f} | Acc {val_acc:.2f}%\")\n",
        "\n",
        "        print(f\"Training completed in {time.time()-start_timestamp:.1f}s\")\n",
        "\n",
        "    def _configure_optimizer(self, lr, decay, optim_choice):\n",
        "        \"\"\"Configures model optimizer\"\"\"\n",
        "        optimizers = {\n",
        "            'adam': optim.Adam,\n",
        "            'nadam': optim.NAdam,\n",
        "            'rmsprop': optim.RMSprop\n",
        "        }\n",
        "        self.optimizer = optimizers[optim_choice.lower()](\n",
        "            self.model.parameters(), lr=lr, weight_decay=decay)\n",
        "\n",
        "    def _save_checkpoint(self):\n",
        "        \"\"\"Saves model state\"\"\"\n",
        "        path = \"/kaggle/working/model\" if self.kaggle_environment else \"model.pth\"\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "\n",
        "    def _log_training_metrics(self, epoch, t_loss, t_acc, v_loss, v_acc):\n",
        "        \"\"\"Handles experiment tracking\"\"\"\n",
        "        wandb.log({\n",
        "            'epoch': epoch+1,\n",
        "            'train_loss': t_loss,\n",
        "            'train_acc': t_acc,\n",
        "            'val_loss': v_loss,\n",
        "            'val_acc': v_acc\n",
        "        })\n",
        "\n",
        "    def run_evaluation(self):\n",
        "        \"\"\"Executes final model evaluation on test set\"\"\"\n",
        "        test_loss, test_acc = self._calculate_metrics(self.model, self.test_loader)\n",
        "        print(f\"Final Test Results - Loss: {test_loss:.2f} | Accuracy: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "q9LGvN5rukam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j1dH4DF_vTRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8V52wuiUvmAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1pKjObHlvcsB"
      }
    }
  ]
}